{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngabo-dev/water-model-peer-group-4/blob/main/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx8_QzWc1r2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhial Majok Riak - Optimization Parameters.\n",
        "\n",
        "I implemented a classification model to predict unsafe water, focusing on ensuring a balanced recall and precision despite imbalanced data. My approach included:\n",
        "\n",
        "Regularizer: L2(0.001) to penalize large weights and reduce overfitting.\n",
        "\n",
        "Dropout Rate: 0.3 applied after each Dense layer.\n",
        "\n",
        "Optimizer: Adam with a learning rate of 0.0004 for fine gradient updates.\n",
        "\n",
        "Early Stopping: Based on val_loss with patience=6 and restore_best_weights=True to prevent overfitting and recover the best weights.\n",
        "\n",
        "Model Evaluation & Compare.\n",
        "\n",
        " My Model Results\n",
        "Metric\tValue\n",
        "Accuracy\t0.6545\n",
        "Precision\t0.5573\n",
        "Recall\t0.5573\n",
        "F1 Score\t0.5573\n",
        "\n",
        "While the accuracy is moderate, all key metrics are balanced, showing that the model does not favor the majority class. This is an improvement from earlier models that had high accuracy but recall and precision at 0.0, meaning they predicted only the majority class.\n",
        "\n",
        " Comparison with Benitha Uwituze’s Model\n",
        "\n",
        "Metric\tBenitha\tNhial\n",
        "Accuracy\t0.9720\t0.6545\n",
        "Precision\t0.9664\t0.5573\n",
        "Recall\t0.8464\t0.5573\n",
        "F1 Score\t0.9024\t0.5573\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Benitha's model outperformed mine in every metric, with strong recall (84.64%) and precision (96.64%).\n",
        "\n",
        "Why my model underperformed:\n",
        "\n",
        "\n",
        "Likely underfitting due to conservative regularization and dropout settings.\n",
        "\n",
        "Smaller learning rate (0.0004) may have slowed convergence.\n",
        "\n",
        "I used a basic L2 regularizer, while Benitha may have used more effective techniques or additional data preprocessing.\n",
        "\n",
        "What I improved over time:\n",
        "\n",
        "\n",
        "Earlier versions of my model had 0% recall and precision despite over 60% accuracy.\n",
        "\n",
        "Now, balanced metrics indicate the model predicts both classes, not just the majority one.\n",
        "\n",
        "\n",
        " Insights and Challenges\n",
        "\n",
        "Insights\n",
        "\n",
        "Overfitting was initially addressed using Dropout and L2 regularization.\n",
        "\n",
        "Early stopping helped prevent continued training on noise.\n",
        "\n",
        "Precision and recall moved from 0.0 to 0.55 — showing progress in learning from both classes.\n",
        "\n",
        "Challenges\n",
        "\n",
        "Initial models were biased toward the majority class, failing to identify unsafe water.\n",
        "\n",
        "Tuning learning rate and dropout required several tests to avoid underfitting.\n",
        "\n",
        "Model performance plateaued, suggesting possible need for feature engineering or resampling techniques.\n",
        "\n",
        "\n",
        "\n",
        "Final Summary Table\n",
        "\n",
        "Train Instance\tEngineer Name\tRegularizer\tOptimizer (LR)\tEarly Stopping Criteria\tDropout Rate\tAccuracy\tF1 Score\tRecall\tPrecision\n",
        "1\tNhial Majok Riak Maketh\tL2 (0.001)\tAdam (lr=0.0004)\tval_loss, patience=6, restore_best_weights=True\t0.3\t0.6545\t0.5573\t0.5573\t0.5573\n",
        "2\tBenitha Uwituze\tUnknown (Likely L2)\tAdam (lr unknown)\tEarly stopping used\tUnknown\t0.9720\t0.9024\t0.8464\t0.9664\n",
        "\n"
      ],
      "metadata": {
        "id": "19A-rHSa3CGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jean Pierre Comparing with Nhial's model**"
      ],
      "metadata": {
        "id": "ZWFxh3N01-Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhial’s model is better because:\n",
        "\n",
        "It uses LeakyReLU activation functions, which help prevent the dying neuron problem and improve learning stability compared to the standard ReLU activation in my model.\n",
        "\n",
        "The dropout rate in Nhial’s model is higher (0.35), which can reduce overfitting more effectively, making the model more robust on unseen data.\n",
        "\n",
        "Nhial employs a lower learning rate and early stopping with patience of 8 epochs, which helps the model avoid overtraining and potentially achieve better generalization than my model that trains for a fixed large number of epochs.\n",
        "\n",
        "However, I noticed that Nhial uses a custom classification threshold of 0.4 to improve recall, which might inflate recall while reducing precision. This could affect the fairness of comparing F1 scores."
      ],
      "metadata": {
        "id": "0xcmWiIi10hV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Placide Imanzi Kabisa\n",
        "\n",
        " Analysis 1 – Comparison With Prince Rurangwa's Model **bold text**\n",
        "I compared my model with Prince Rurangwa’s. We used different hyperparameters, which led to different results in performance. Here’s what I noticed:\n",
        " Precision\n",
        "Mine: 0.704\n",
        "\n",
        "\n",
        "Prince: 0.653\n",
        "\n",
        "\n",
        "My model is clearly better here. It made fewer mistakes when predicting unsafe water. I think the stronger regularization (L1 + L2) and higher dropout (0.4) made it more careful and reduced overconfident predictions. Prince used only L1 and a lower dropout (0.3), which probably gave his model more flexibility but made it a bit less accurate in those unsafe predictions.\n",
        "\n",
        " Recall\n",
        "Mine: 0.269\n",
        "\n",
        "\n",
        "Prince: 0.333\n",
        "\n",
        "\n",
        "This is where Prince’s model did better. His model caught more of the unsafe water cases, which is more important in a real-life setting. I suspect that my model was too strict because of the strong regularization and the Adagrad optimizer with a high learning rate. His use of RMSprop and lighter regularization helped his model be more open and detect more unsafe samples.\n",
        "\n",
        " F1 Score\n",
        "Mine: 0.389\n",
        "\n",
        "\n",
        "Prince: 0.441\n",
        "\n",
        "\n",
        "Because his model had better recall while still keeping decent precision, the F1 score came out better overall. Mine had high precision but recall was too low, which dragged the F1 score down. So Prince's model was more balanced.\n",
        "Accuracy\n",
        "Mine: 0.681\n",
        "\n",
        "\n",
        "Prince: 0.681\n",
        "\n",
        "\n",
        "We both had the same accuracy, but I don’t think this tells the full story. It’s easy to get decent accuracy if the model just predicts most samples as safe. What really matters here is whether the model correctly catches unsafe water and Prince’s model did better on that.\n",
        "Conclusion\n",
        "Best model: Prince’s\n",
        "He got the better balance overall and did a better job at finding unsafe water, which is the main goal.\n",
        "\n",
        "\n",
        "His use of RMSprop and milder regularization helped his model learn better without overfitting.\n",
        "\n",
        "\n",
        " Weaker model: Mine\n",
        "Even though I had better precision, the recall was too low. That means my model misses too many unsafe samples  not great for this kind of task.\n",
        "\n",
        "\n",
        "I probably over-regularized it, and Adagrad might not have been the best choice here.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Analysis 2 – Comparison With Uwituze Benitha’s Model**\n",
        "Now I’m comparing my model with Benitha’s. She used a different optimizer (Adam instead of Adagrad), only L2 regularization (not L1 or both), and a smaller learning rate. Here's how we compare:\n",
        "Precision\n",
        "Mine: 0.704\n",
        "\n",
        "\n",
        "Benitha: 0.6591\n",
        "\n",
        "\n",
        "I did better here again. My model was more precise when predicting unsafe water. That means it gave fewer false alarms. I think this comes from using both L1 and L2 regularization and a higher dropout (0.4), which made my model more careful. Benitha only used L2 and a dropout of 0.3, so her model was more flexible  which probably helped with recall but slightly lowered precision.\n",
        "\n",
        " Recall\n",
        "Mine: 0.2688\n",
        "\n",
        "\n",
        "Benitha: 0.3118\n",
        "\n",
        "\n",
        "Her model performed better here. It caught more of the unsafe water cases. I believe her use of Adam optimizer with a small learning rate (0.0005) helped the model learn slower and more steadily, which probably allowed it to find more real positives. My Adagrad optimizer with a learning rate of 0.01 might have caused my model to miss important patterns, especially for the positive class.\n",
        " F1 Score\n",
        "Mine: 0.3891\n",
        "\n",
        "\n",
        "Benitha: 0.4234\n",
        "\n",
        "\n",
        "F1 score favors Benitha’s model again. It shows that she found a better balance between precision and recall. I had higher precision, but my low recall pulled my F1 down. Benitha’s model, while not perfect, didn’t sacrifice recall as much as mine did  and that made her F1 stronger.\n",
        " Accuracy\n",
        "Mine: 0.6809\n",
        "\n",
        "\n",
        "Benitha: 0.6789\n",
        "\n",
        "\n",
        "Very close. We’re almost tied here. But again, accuracy isn’t that meaningful in this case. A model can have high accuracy even if it misses unsafe samples, so I focused more on recall and F1 when judging the performance.\n",
        "Conclusion\n",
        " Best model: Benitha’s\n",
        "Her F1 score and recall were both better, meaning she found more unsafe water while still keeping good precision.\n",
        "\n",
        "\n",
        "Her use of Adam with a very low learning rate helped the model learn more gradually and carefully.\n",
        "\n",
        "\n",
        "She balanced generalization well with L2 regularization and a decent dropout.\n",
        "\n",
        "\n",
        "   Weaker model: Mine\n",
        "Precision was higher, but recall was too low, so overall F1 dropped.\n",
        "\n",
        "\n",
        "The combination of Adagrad, high learning rate, strong regularization, and high dropout probably made the model too conservative  missing too many unsafe samples.\n",
        "\n",
        "\n",
        "**Insights from Experiments & Challenges Faced**\n",
        "From running my experiments, I learned that using a combination of L1 and L2 regularization together with a high dropout rate (0.4) made my model too conservative. It became less flexible and struggled to learn the patterns properly. This is probably why my recall was low — the model was being too cautious and missed a lot of unsafe water samples.\n",
        "I also realized that Adagrad wasn’t the best optimizer for this task. It tends to perform worse on sparse or imbalanced data because it shrinks the learning rate too quickly, which might explain why my model didn’t improve much after a certain point.\n",
        "One of the biggest challenges I faced was choosing the right optimizer and learning rate. Small changes made a big difference in results, and it was hard to know what combination would work best without trying a lot of options.\n",
        "Another challenge was balancing precision and recall. Sometimes when I improved one, the other would drop. It took a lot of trial and error to understand how different settings affected both sides of the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFhboypR89Eb"
      }
    }
  ]
}