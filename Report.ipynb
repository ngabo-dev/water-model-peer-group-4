{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngabo-dev/water-model-peer-group-4/blob/main/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx8_QzWc1r2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights and Challenges – Nhial Majok Riak Maketh\n",
        "Insights:\n",
        "\n",
        "Model Balance Achieved: After experimenting with various dropout rates and learning rates, I discovered that a smaller learning rate (0.0004) combined with L2 regularization (0.001) resulted in a more stable training process. This led to significantly improved recall (from 0.125 to 0.5573) and a balanced F1 score.\n",
        "\n",
        "Dropout Rate Sweet Spot: Using a dropout rate of 0.3 provided enough regularization to prevent overfitting, while still allowing the model to learn effectively from training data.\n",
        "\n",
        "Early Stopping Effectiveness: Early stopping with a patience of 6 helped avoid overfitting and stopped training when performance stopped improving on validation data.\n",
        "\n",
        "Threshold Tuning Impact: Adjusting the decision threshold (by interpreting predicted probabilities better) helped improve F1 by making the model better at identifying minority class (potable water).\n",
        "\n",
        "Challenges:\n",
        "\n",
        "Initial Low Recall: Early versions of the model had very low recall (0.12–0.35), meaning the model was missing many actual positives. It took multiple tuning cycles to correct this imbalance.\n",
        "\n",
        "Learning Curve for Regularization: Understanding how L2 regularization interacts with dropout was difficult at first. Using both carefully together significantly improved performance.\n",
        "\n",
        "Class Imbalance Effects: While not extreme, class imbalance in the dataset caused the model to favor the majority class initially. I had to work around this with architecture and loss tuning.\n",
        "\n",
        "Model Summary Table:\n",
        "Train Instance\tEngineer Name\tRegularizer\tOptimizer (LR)\tEarly Stopping Criteria\tDropout Rate\tAccuracy\tF1 Score\tRecall\tPrecision\n",
        "1\tNhial Majok Riak Maketh"
      ],
      "metadata": {
        "id": "19A-rHSa3CGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jean Pierre Comparing with Nhial's model**"
      ],
      "metadata": {
        "id": "ZWFxh3N01-Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhial’s model is better because:\n",
        "\n",
        "It uses LeakyReLU activation functions, which help prevent the dying neuron problem and improve learning stability compared to the standard ReLU activation in my model.\n",
        "\n",
        "The dropout rate in Nhial’s model is higher (0.35), which can reduce overfitting more effectively, making the model more robust on unseen data.\n",
        "\n",
        "Nhial employs a lower learning rate and early stopping with patience of 8 epochs, which helps the model avoid overtraining and potentially achieve better generalization than my model that trains for a fixed large number of epochs.\n",
        "\n",
        "However, I noticed that Nhial uses a custom classification threshold of 0.4 to improve recall, which might inflate recall while reducing precision. This could affect the fairness of comparing F1 scores."
      ],
      "metadata": {
        "id": "0xcmWiIi10hV"
      }
    }
  ]
}