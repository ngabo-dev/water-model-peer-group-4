{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngabo-dev/water-model-peer-group-4/blob/main/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx8_QzWc1r2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhial Majok Riak - Optimization Parameters.\n",
        "\n",
        "I implemented a classification model to predict unsafe water, focusing on ensuring a balanced recall and precision despite imbalanced data. My approach included:\n",
        "\n",
        "Regularizer: L2(0.001) to penalize large weights and reduce overfitting.\n",
        "\n",
        "Dropout Rate: 0.3 applied after each Dense layer.\n",
        "\n",
        "Optimizer: Adam with a learning rate of 0.0004 for fine gradient updates.\n",
        "\n",
        "Early Stopping: Based on val_loss with patience=6 and restore_best_weights=True to prevent overfitting and recover the best weights.\n",
        "\n",
        "Model Evaluation & Compare.\n",
        "\n",
        " My Model Results\n",
        "Metric\tValue\n",
        "Accuracy\t0.6545\n",
        "Precision\t0.5573\n",
        "Recall\t0.5573\n",
        "F1 Score\t0.5573\n",
        "\n",
        "While the accuracy is moderate, all key metrics are balanced, showing that the model does not favor the majority class. This is an improvement from earlier models that had high accuracy but recall and precision at 0.0, meaning they predicted only the majority class.\n",
        "\n",
        " Comparison with Benitha Uwituze’s Model\n",
        "\n",
        "Metric\tBenitha\tNhial\n",
        "Accuracy\t0.9720\t0.6545\n",
        "Precision\t0.9664\t0.5573\n",
        "Recall\t0.8464\t0.5573\n",
        "F1 Score\t0.9024\t0.5573\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Benitha's model outperformed mine in every metric, with strong recall (84.64%) and precision (96.64%).\n",
        "\n",
        "Why my model underperformed:\n",
        "\n",
        "\n",
        "Likely underfitting due to conservative regularization and dropout settings.\n",
        "\n",
        "Smaller learning rate (0.0004) may have slowed convergence.\n",
        "\n",
        "I used a basic L2 regularizer, while Benitha may have used more effective techniques or additional data preprocessing.\n",
        "\n",
        "What I improved over time:\n",
        "\n",
        "\n",
        "Earlier versions of my model had 0% recall and precision despite over 60% accuracy.\n",
        "\n",
        "Now, balanced metrics indicate the model predicts both classes, not just the majority one.\n",
        "\n",
        "\n",
        " Insights and Challenges\n",
        "\n",
        "Insights\n",
        "\n",
        "Overfitting was initially addressed using Dropout and L2 regularization.\n",
        "\n",
        "Early stopping helped prevent continued training on noise.\n",
        "\n",
        "Precision and recall moved from 0.0 to 0.55 — showing progress in learning from both classes.\n",
        "\n",
        "Challenges\n",
        "\n",
        "Initial models were biased toward the majority class, failing to identify unsafe water.\n",
        "\n",
        "Tuning learning rate and dropout required several tests to avoid underfitting.\n",
        "\n",
        "Model performance plateaued, suggesting possible need for feature engineering or resampling techniques.\n",
        "\n",
        "\n",
        "\n",
        "Final Summary Table\n",
        "\n",
        "Train Instance\tEngineer Name\tRegularizer\tOptimizer (LR)\tEarly Stopping Criteria\tDropout Rate\tAccuracy\tF1 Score\tRecall\tPrecision\n",
        "1\tNhial Majok Riak Maketh\tL2 (0.001)\tAdam (lr=0.0004)\tval_loss, patience=6, restore_best_weights=True\t0.3\t0.6545\t0.5573\t0.5573\t0.5573\n",
        "2\tBenitha Uwituze\tUnknown (Likely L2)\tAdam (lr unknown)\tEarly stopping used\tUnknown\t0.9720\t0.9024\t0.8464\t0.9664\n",
        "\n"
      ],
      "metadata": {
        "id": "19A-rHSa3CGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jean Pierre Comparing with Nhial's model**"
      ],
      "metadata": {
        "id": "ZWFxh3N01-Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhial’s model is better because:\n",
        "\n",
        "It uses LeakyReLU activation functions, which help prevent the dying neuron problem and improve learning stability compared to the standard ReLU activation in my model.\n",
        "\n",
        "The dropout rate in Nhial’s model is higher (0.35), which can reduce overfitting more effectively, making the model more robust on unseen data.\n",
        "\n",
        "Nhial employs a lower learning rate and early stopping with patience of 8 epochs, which helps the model avoid overtraining and potentially achieve better generalization than my model that trains for a fixed large number of epochs.\n",
        "\n",
        "However, I noticed that Nhial uses a custom classification threshold of 0.4 to improve recall, which might inflate recall while reducing precision. This could affect the fairness of comparing F1 scores."
      ],
      "metadata": {
        "id": "0xcmWiIi10hV"
      }
    }
  ]
}